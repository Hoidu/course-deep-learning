{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L3 - Data Processing",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pA1nD/course-deep-learning/blob/master/L3_Data_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJ4T8ku38huH",
        "colab_type": "text"
      },
      "source": [
        "# Data input pipelines\n",
        "\n",
        "Loading and working with data. Read data from various formats and store it as tf.data Dataset\n",
        "\n",
        "And make sure to check how to [Build TensorFlow input pipelines](https://www.tensorflow.org/guide/data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWzZi8ny85xH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV3wR7ktEy-i",
        "colab_type": "text"
      },
      "source": [
        "# Loading Data\n",
        "\n",
        "There are more examples available at [https://www.tensorflow.org/tutorials/load_data/csv](https://www.tensorflow.org/tutorials/load_data/csv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YynDurNG-o8z",
        "colab_type": "text"
      },
      "source": [
        "## From CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3ajdfbC-sWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n",
        "TEST_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\"\n",
        "\n",
        "train_file_path = tf.keras.utils.get_file(\"train.csv\", TRAIN_DATA_URL)\n",
        "test_file_path = tf.keras.utils.get_file(\"eval.csv\", TEST_DATA_URL)\n",
        "\n",
        "# you could also load this via pandas. As you see further down."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOqo9sEGAAWM",
        "colab_type": "text"
      },
      "source": [
        "Print the first few lines from the csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ-W1peo__LH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head {train_file_path}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzHeHzzDAErq",
        "colab_type": "text"
      },
      "source": [
        "Load the data via `tf.data.experimental.make_csv_dataset()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7bBL2chAXGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_dataset(file_path, **kwargs):\n",
        "  dataset = tf.data.experimental.make_csv_dataset(\n",
        "      file_path,\n",
        "      batch_size=5, # Artificially small to make examples easier to show.\n",
        "      label_name='survived', # The only column you need to identify explicitly is the one with the value that the model is intended to predict.\n",
        "      na_value=\"?\",\n",
        "      num_epochs=1, # will cycle through dataset infinitely if undefined\n",
        "      ignore_errors=True, \n",
        "      **kwargs)\n",
        "  return dataset\n",
        "\n",
        "raw_train_data = get_dataset(train_file_path)\n",
        "raw_test_data = get_dataset(test_file_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kctSwZD6AqLx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_batch(dataset):\n",
        "  for batch, label in dataset.take(1):\n",
        "    for key, value in batch.items():\n",
        "      print(\"{:20s}: {}\".format(key,value.numpy()))\n",
        "show_batch(raw_train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9awqUwXBS5x",
        "colab_type": "text"
      },
      "source": [
        "To scope training only to a few available colums, pass them as `select_colums`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voCgXJ56BRSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SELECT_COLUMNS = ['survived', 'age', 'n_siblings_spouses', 'class', 'deck', 'alone']\n",
        "\n",
        "temp_dataset = get_dataset(train_file_path, select_columns=SELECT_COLUMNS)\n",
        "\n",
        "show_batch(temp_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wm_59xEZ8tOd",
        "colab_type": "text"
      },
      "source": [
        "## From Numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krXpxgFA-PTv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test = tf.keras.datasets.fashion_mnist.load_data()\n",
        "images, labels = train\n",
        "images = images/255\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((images, labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81MtdARg8xfz",
        "colab_type": "text"
      },
      "source": [
        "## From Pandas DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R44pk_q8sYB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csv_file = tf.keras.utils.get_file('heart.csv', 'https://storage.googleapis.com/applied-dl/heart.csv')\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "df.head()\n",
        "\n",
        "# we could see using df.dtypes that thal is an object.\n",
        "# Convert thal column which is an object in the dataframe to a discrete numerical value.\n",
        "\n",
        "df['thal'] = pd.Categorical(df['thal'])\n",
        "df['thal'] = df.thal.cat.codes\n",
        "\n",
        "target = df.pop('target')\n",
        "dataset = tf.data.Dataset.from_tensor_slices((df.values, target.values))\n",
        "\n",
        "# shuffle and batch\n",
        "train_dataset = dataset.shuffle(len(df)).batch(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rgoz3CK1E1Po",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing Data\n",
        "\n",
        "The following section shows:\n",
        "- Batching\n",
        "- Repeating for multiple epochs\n",
        "- shffling\n",
        "\n",
        "For further pre-processing check [the preprocessing guide](https://www.tensorflow.org/guide/data#preprocessing_data)\n",
        "\n",
        "and have a look [here for data preprocessing](https://www.tensorflow.org/tutorials/load_data/csv#data_preprocessing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Lg5RFX6E3bU",
        "colab_type": "text"
      },
      "source": [
        "## Batching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ax2ryV-E4Lt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batched_dataset = dataset.batch(7, drop_remainder=True) # drop_remainder=True will drop the last batch if it is not a full batch."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSQYy9dxFQQa",
        "colab_type": "text"
      },
      "source": [
        "## Repeating for Multiple Epochs\n",
        "\n",
        "Applying the Dataset.repeat() transformation with no arguments will repeat the input indefinitely.\n",
        "\n",
        "The Dataset.repeat transformation concatenates its arguments without signaling the end of one epoch and the beginning of the next epoch. Because of this a Dataset.batch applied after Dataset.repeat will yield batches that straddle epoch boundaries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw0oWwZ0FTGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "titanic_lines.repeat(3).batch(128) # will have last batch of epoch 3 with a smaller size\n",
        "titanic_lines.batch(128).repeat(3) # will create smaller batches at every last batch of an epoch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVjWaWygFsGK",
        "colab_type": "text"
      },
      "source": [
        "## Shuffling\n",
        "\n",
        "The Dataset.shuffle() transformation maintains a fixed-size buffer and chooses the next element uniformly at random from that buffer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LNNKvcsGFhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset.shuffle(buffer_size=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfedFE5IGGp8",
        "colab_type": "text"
      },
      "source": [
        "As with Dataset.batch the order relative to Dataset.repeat matters.\n",
        "\n",
        "Dataset.shuffle doesn't signal the end of an epoch until the shuffle buffer is empty. So a shuffle placed before a repeat will show every element of one epoch before moving to the next:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2mW2BrDGIsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset.shuffle(buffer_size=100).repeat(2).batch(10)\n",
        "# not the same as\n",
        "dataset.repeat(2).shuffle(buffer_size=100).batch(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S440_S3rHUzK",
        "colab_type": "text"
      },
      "source": [
        "# License\n",
        "\n",
        "Copyright 2019 The TensorFlow Authors and 2020 Björn Schmidtke for GSERM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bLSRGRqHcNV",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}